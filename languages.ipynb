{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_languages(\n",
    "    path: str, size: int = 8, seperator: str = \"language\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df_ = pd.read_csv(path)\n",
    "    df_ = fix_languages(df_)\n",
    "    df_[\"language\"] = df_[\"language\"].apply(lambda x: x.upper().strip())\n",
    "    df_[\"language\"] = df_[\"language\"].apply(lambda x: translation(str(x)))\n",
    "    total_langs_df = df_.groupby(by=\"user_id\", as_index=False).agg(\n",
    "        total_langs=(\n",
    "            \"language\",\n",
    "            lambda x: len([s.strip() for s in \", \".join(x.unique()).split(\",\")]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=size, stop_words=stopwords.words(\"english\"), ngram_range=(1, 1)\n",
    "    )\n",
    "\n",
    "    lang_grouped_df = df_.groupby(by=\"user_id\", as_index=False).agg(\n",
    "        {\"language\": lambda x: \" \".join(x.unique())}\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            vectorizer.fit_transform(lang_grouped_df[\"language\"]).toarray(),\n",
    "            columns=[f\"language_{str(f)}\" for f in vectorizer.get_feature_names()],\n",
    "        )\n",
    "        .assign(user_id=lang_grouped_df[\"user_id\"])\n",
    "        .merge(total_langs_df, on=[\"user_id\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "\n",
    "#\n",
    "# df_ = df_.drop_duplicates()\n",
    "#\n",
    "# total_langs_df = df_.groupby(by=\"user_id\", as_index=False).agg(\n",
    "#    total_langs=(\n",
    "#        \"language\",\n",
    "#        lambda x: len([s.strip() for s in \", \".join(x.unique()).split(\",\")]),\n",
    "#    )\n",
    "# )\n",
    "#\n",
    "# lang_grouped_df = df_.groupby(by = 'user_id', as_index=False).agg({'language': lambda x: ' '.join(x.unique())})\n",
    "#\n",
    "# vectorizer = CountVectorizer(max_features=size,\n",
    "#                             stop_words=stopwords.words(\"english\"),\n",
    "#                             ngram_range=(1,1))\n",
    "###\n",
    "#\n",
    "##prof_grouped_df = df_.groupby(by = 'user_id', as_index=False).agg({'proficiency': lambda x: ' '.join(x.unique())})\n",
    "##vectorizer = CountVectorizer(max_features=5,\n",
    "##                             stop_words=stopwords.words(\"english\"),\n",
    "##                             ngram_range=(1,1))\n",
    "##\n",
    "##prof_df = pd.DataFrame(\n",
    "##    vectorizer.fit_transform(prof_grouped_df[\"proficiency\"]).toarray(),\n",
    "##    columns=[f'proficiency_{str(f)}' for f in vectorizer.get_feature_names()],\n",
    "##).assign(user_id = prof_grouped_df['user_id'])\n",
    "#\n",
    "# return pd.DataFrame(\n",
    "#    vectorizer.fit_transform(lang_grouped_df[\"language\"]).toarray(),\n",
    "#    columns=[f'language_{str(f)}' for f in vectorizer.get_feature_names()],\n",
    "# ).assign(user_id = lang_grouped_df['user_id']).merge(total_langs_df, on = ['user_id'], how = 'left')\n",
    "\n",
    "# df_ = df_.drop_duplicates()\n",
    "# most_freq_langs = df_['language'].value_counts().keys()[:size].tolist()\n",
    "# grouped = df_.groupby(by='user_id', as_index=False).agg({'language': 'unique'})\n",
    "# for lang in tqdm(most_freq_langs):\n",
    "#    grouped[f'{seperator}_{lang}'] = grouped['language'].apply(lambda x: 1 if lang in x else 0)\n",
    "#\n",
    "# grouped = grouped.merge(df_.groupby(by='user_id', as_index=False).agg(\n",
    "#    total_languages=('language', 'count')), on=['user_id'], how='left')\n",
    "# grouped = grouped.drop(columns=['language'], axis=1)\n",
    "\n",
    "# prof_df = df_.groupby(by = ['user_id', 'proficiency'], as_index = False).agg({'language': 'count'})\n",
    "# pv = pd.pivot_table(prof_df, values='language', index=['user_id'], columns=['proficiency'], aggfunc=np.sum, fill_value=0).reset_index()\n",
    "# pv.columns.name = None\n",
    "# pv.columns = pv.columns.map(lambda x: f'proficiency_{str(x)}' if x != 'user_id' else x)\n",
    "\n",
    "# grouped = grouped.merge(pv, on = ['user_id'], how = 'left')\n",
    "\n",
    "# return grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_languages('../../../datasets/garanti-bbva-data-camp/languages.csv')\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['language'].str.contains('Deutsch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['language'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.loc[df[\"language\"] == \"İşaret Dilleri\", \"language\"] = \"Sign Languages\"\n",
      "df.loc[df[\"language\"] == \"İbranice\", \"language\"] = \"Hebrew\"\n"
     ]
    }
   ],
   "source": [
    "#for i in df.loc[df['language'].str.contains('İ'), 'language'].value_counts()[:50].keys():\n",
    "#    translated = GoogleTranslator(source='auto', target='en').translate(i)\n",
    "#    if df.loc[df['language'] == translated].shape[0] != 0:\n",
    "#        print(f'df.loc[df[\"language\"] == \"{i}\", \"language\"] = \"{translated.title()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.merge(df, on = ['user_id'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_map(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35371f657a4770bb32286f2b1d2d1b12c4bc2be917cf11a1e3547ec3dbe6c433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
